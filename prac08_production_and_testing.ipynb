{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 8: Production and Testing\n",
    "\n",
    "### In this practical note\n",
    "1. [Training a basic model]\n",
    "2. [Data pipeline]\n",
    "3. [Serialisation]\n",
    "4. [Other tips]\n",
    "---\n",
    "\n",
    "### Important Changelog:\n",
    "\n",
    "The practical note for this week introduces you to the practical aspect of Python machine learning models in production environment. Production environment is where a model gets new data and makes predictions to help make decision. There are differences in focus between models in training phase compared to models in production phase. This topic is a seldom one to discuss in an university unit, but I believe it is equally as important as other theoretical components.\n",
    "\n",
    "We have learned how to perform preprocessing and cleaning on a dataset, building and validating model, selecting important features and conducting both supervised and unsupervised data mining tasks. This practical will introduce you on concepts beyond the training, what happened after your model is finished and how to ensure your model is production ready.\n",
    "\n",
    "To ensure sufficient time to finish your workload, anything discussed in this week will not be graded unit components (assignment/exam).\n",
    "\n",
    "## 1. Training a basic model\n",
    "\n",
    "To start this practical, we will train a LogisticRegression model on the `veteran.csv` dataset that you are very familiar with. Follow the next cells for step-by-step instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.593510324484\n",
      "Test accuracy: 0.562284927736\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.57      0.57      1453\n",
      "          1       0.56      0.55      0.56      1453\n",
      "\n",
      "avg / total       0.56      0.56      0.56      2906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from dm_tools import data_prep\n",
    "\n",
    "# preprocessing step\n",
    "df = data_prep()\n",
    "\n",
    "# set the random seed - consistent\n",
    "rs = 10\n",
    "\n",
    "# train test split\n",
    "y = df['TargetB']\n",
    "X = df.drop(['TargetB'], axis=1)\n",
    "X_mat = X.as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.3, stratify=y, random_state=rs)\n",
    "\n",
    "# initialise a standard scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# learn the mean and std.dev of variables from training data\n",
    "# then use the learned values to transform training data\n",
    "X_train = scaler.fit_transform(X_train, y_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# initial model\n",
    "init_model = LogisticRegression(random_state=rs)\n",
    "\n",
    "# fit it to training data\n",
    "init_model.fit(X_train, y_train)\n",
    "\n",
    "# training and test accuracy\n",
    "print(\"Train accuracy:\", init_model.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", init_model.score(X_test, y_test))\n",
    "\n",
    "# classification report on test data\n",
    "y_pred = init_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen previously, the model performed on acceptable level on both training and test data. Assume that after many experiments, we have concluded that this model performs the best on all settings. We will use this model as our deployment model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Pipeline\n",
    "\n",
    "An important step before deploying our model into production system is to ensure all steps performed in data preparation and feature engineering are consistent and complete. With many steps performed in before the data even gets to the model, it is common to make mistake and lose important actions through data leakage heading somewhere.\n",
    "\n",
    "There are three major classes used in building `sklearn` data pipelines.\n",
    "1. `BaseEstimator` and `TransformerMixin`, used as parent classes to build Transformers. Transformers are classes responsible for applying transformation steps onto the dataset.\n",
    "1. `Pipeline`. Pipelines are a method to pack each step in your logic sequentially into one function call, making it easier to perform training and predictions. Given 3 transformers A, B and C, a Pipeline will run them in order of A->B->C. With a Pipeline, you can ensure consistent steps are applied into to training, validation, test and future data. Pipeline class also implements fit, transform and prediction functions, automating many of the function calls.\n",
    "2. `FeatureUnion`. While pipelines run each step sequentially, `FeatureUnion` join each step in paralel. Instead of A->B->C, FeatureUnion will return a larger set of results from [A, B, C].\n",
    "\n",
    "To start with data pipeline, import classes above as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion, make_union\n",
    "from sklearn.base import TransformerMixin, BaseEstimator, clone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help with applying transformation steps onto our dataframe, we will create a helper transformer as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Apply given transformation.\"\"\"\n",
    "    def __init__(self, trans_func, untrans_func, columns):\n",
    "        self.transform_func = trans_func\n",
    "        self.inverse_transform_func = untrans_func\n",
    "        self.cols = columns\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        x = self._get_selection(x)\n",
    "        return self.transform_func(x) if callable(self.transform_func) else x\n",
    "\n",
    "    def inverse_transform(self, x):\n",
    "        return self.inverse_transform_func(x) \\\n",
    "            if callable(self.inverse_transform_func) else x\n",
    "\n",
    "    def _get_selection(self, df):\n",
    "        assert isinstance(df, pd.DataFrame)\n",
    "        return df[self.cols]\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self.cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we could start joining transformation operations together. Find the code and their explaination as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, Imputer  # import imputer and scaler\n",
    "\n",
    "# get the original dataset as reference\n",
    "df2 = pd.read_csv('datasets/veteran.csv')\n",
    "\n",
    "# drop unnecessary columns\n",
    "dropped = df2.drop(['ID', 'TargetD', 'TargetB'], axis=1)\n",
    "\n",
    "all_columns = dropped.columns.tolist()\n",
    "obj_columns = dropped.columns[dropped.dtypes == object].tolist()  # separate categorical and numerical columns\n",
    "trans_columns = ['DemCluster', 'DemHomeOwner', 'DemMedIncome']  # set aside columns with special processing steps\n",
    "\n",
    "int_columns = list(set(all_columns) - set(obj_columns) - set(trans_columns))\n",
    "obj_columns = list(set(obj_columns) - set(trans_columns))\n",
    "\n",
    "# create your first pipeline, transforming `DemCluster` into categorical and running one hot encoding on all categorical columns\n",
    "string_pipe = make_pipeline(\n",
    "    FeatureUnion(\n",
    "        [('demcluster', SimpleTransformer(lambda x: pd.get_dummies(x.astype(str)), None, ['DemCluster'])),\n",
    "        ('identity', SimpleTransformer(lambda x: pd.get_dummies(x), None, obj_columns))]\n",
    "    )\n",
    ")\n",
    "\n",
    "# help to remove noise in `DemMedIncome`\n",
    "noise_pipe = SimpleTransformer(lambda x: x.replace(0, np.nan), None, ['DemMedIncome'])\n",
    "\n",
    "# union both pipes above, impute and scale the dataframe into a numpy array\n",
    "pipeline = make_pipeline(\n",
    "    FeatureUnion([('stringpipe', string_pipe), ('noisepipe', noise_pipe), ('ints', SimpleTransformer(None, None, int_columns))]),\n",
    "    Imputer(),\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(random_state=rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a pipeline set up, you could easily apply similar preprocessing steps for both training and test data. Apply the pipeline into `df2` and see how it applies all data preprocessing steps and training for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline training score:  0.588656387665\n",
      "Pipeline test score:  0.571841453344\n"
     ]
    }
   ],
   "source": [
    "# create X and Y\n",
    "df_X = df2.drop(['TargetB'], axis=1)\n",
    "df_y = df2['TargetB']\n",
    "\n",
    "# notice that df_X, df_X_train and df_X_test are still pandas DataFrame instead of numpy array\n",
    "df_X_train, df_X_test, df_y_train, df_y_test = train_test_split(df_X, df_y, stratify=df_y, random_state=rs)\n",
    "\n",
    "# apply transformation and train model in one go\n",
    "pipeline.fit(df_X, df_y)\n",
    "\n",
    "# score pipeline on training and test data\n",
    "print(\"Pipeline training score: \", pipeline.score(df_X_train, df_y_train))\n",
    "print(\"Pipeline test score: \", pipeline.score(df_X_test, df_y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Serialisation\n",
    "\n",
    "Once you have your pipeline ready, you could serialise the model and all pipeline components into a binary file. Binary file allow easy versioning and easy deployment through file transfer. `sklearn` official documentation recommends using `pickle` to serialise pipeline and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  8.55294819e-02   5.72905765e-02   9.60262361e-02  -7.68268037e-02\n",
      "   -5.03317638e-02  -3.99429982e-02   7.37564612e-02  -5.93071375e-02\n",
      "   -1.68332723e-01   2.80447649e-01  -1.39864836e-01   5.06139983e-02\n",
      "    1.97820127e-01   4.77841914e-02   9.49498242e-02  -4.38512559e-01\n",
      "    1.10047871e-01   5.51011404e-02   2.70004445e-02   1.25856209e-01\n",
      "    4.53598078e-02  -2.50445045e-02  -2.30582374e-02   7.76478322e-02\n",
      "   -2.66611375e-02  -7.17073881e-03   2.82414971e-02   5.45915187e-05\n",
      "    4.95292495e-02   2.50351269e-02  -5.18782525e-02   2.46976545e-02\n",
      "    2.03661586e-03   4.34903010e-03  -1.69243519e-04  -1.34790895e-03\n",
      "   -2.41957340e-02   1.54527250e-02   2.44627356e-02  -5.96265462e-04\n",
      "    9.41790072e-03   1.79343011e-02  -3.78651082e-02  -1.01814670e-02\n",
      "   -1.30084774e-02   1.83655032e-02  -7.87823668e-03   8.94038392e-04\n",
      "   -3.89726384e-04   1.77516004e-02   1.19268189e-02   2.45853350e-02\n",
      "   -5.11888957e-02   1.11627368e-02  -4.77942174e-02  -1.00081318e-02\n",
      "   -3.03296908e-03   3.13872418e-02  -1.14767292e-03  -1.78770954e-02\n",
      "    3.76352505e-02   1.77136706e-02   2.15441246e-02   4.20433112e-02\n",
      "   -3.26689324e-02   4.23613050e-02  -1.14826888e-02  -2.38149054e-02\n",
      "   -4.50399335e-02   4.15010922e-02  -5.28358478e-02  -8.82769886e-03\n",
      "   -2.79017387e-02  -1.95714531e-02   7.06573697e-03  -1.79368199e-02\n",
      "   -3.52741210e-02   2.68294262e-02  -3.67792906e-02   8.25559246e-03\n",
      "   -2.19038475e-02  -2.22317776e-02   5.20675074e-03   1.87282270e-03\n",
      "   -1.53757147e-02]]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# show model coefficient before pickled\n",
    "print(init_model.coef_)\n",
    "\n",
    "# dump to file named model.pkl, written in binary mode\n",
    "pickle.dump(file=open('model.pkl', 'wb'), obj=init_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model/pipeline is serialise, you could reload it using this code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  8.55294819e-02   5.72905765e-02   9.60262361e-02  -7.68268037e-02\n",
      "   -5.03317638e-02  -3.99429982e-02   7.37564612e-02  -5.93071375e-02\n",
      "   -1.68332723e-01   2.80447649e-01  -1.39864836e-01   5.06139983e-02\n",
      "    1.97820127e-01   4.77841914e-02   9.49498242e-02  -4.38512559e-01\n",
      "    1.10047871e-01   5.51011404e-02   2.70004445e-02   1.25856209e-01\n",
      "    4.53598078e-02  -2.50445045e-02  -2.30582374e-02   7.76478322e-02\n",
      "   -2.66611375e-02  -7.17073881e-03   2.82414971e-02   5.45915187e-05\n",
      "    4.95292495e-02   2.50351269e-02  -5.18782525e-02   2.46976545e-02\n",
      "    2.03661586e-03   4.34903010e-03  -1.69243519e-04  -1.34790895e-03\n",
      "   -2.41957340e-02   1.54527250e-02   2.44627356e-02  -5.96265462e-04\n",
      "    9.41790072e-03   1.79343011e-02  -3.78651082e-02  -1.01814670e-02\n",
      "   -1.30084774e-02   1.83655032e-02  -7.87823668e-03   8.94038392e-04\n",
      "   -3.89726384e-04   1.77516004e-02   1.19268189e-02   2.45853350e-02\n",
      "   -5.11888957e-02   1.11627368e-02  -4.77942174e-02  -1.00081318e-02\n",
      "   -3.03296908e-03   3.13872418e-02  -1.14767292e-03  -1.78770954e-02\n",
      "    3.76352505e-02   1.77136706e-02   2.15441246e-02   4.20433112e-02\n",
      "   -3.26689324e-02   4.23613050e-02  -1.14826888e-02  -2.38149054e-02\n",
      "   -4.50399335e-02   4.15010922e-02  -5.28358478e-02  -8.82769886e-03\n",
      "   -2.79017387e-02  -1.95714531e-02   7.06573697e-03  -1.79368199e-02\n",
      "   -3.52741210e-02   2.68294262e-02  -3.67792906e-02   8.25559246e-03\n",
      "   -2.19038475e-02  -2.22317776e-02   5.20675074e-03   1.87282270e-03\n",
      "   -1.53757147e-02]]\n"
     ]
    }
   ],
   "source": [
    "pickled_model = pickle.load(file=open('model.pkl', 'rb'))\n",
    "\n",
    "print(pickled_model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is serialised and stored perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Other tips\n",
    "\n",
    "This section discusses various performance tips that you could use to help your model works better in production setting.\n",
    "\n",
    "### 4.1. Batch prediction\n",
    "\n",
    "In many production systems, real-time atomic predictions is not required. Often prediction tasks can be stashed and delayed to be then processed in batch basis. Batch processing allows the model to take advantage of vectorised computations and SIMD instructions (single instruction, multiple data), which speeds up computational performance significantly.\n",
    "\n",
    "To refer a real-life example, GoCardless, a YCombinator funded startup which focuses on cashless payments, uses machine learning models to perform fraud detection activities. Most payment transactions in the company take 1-2 days, therefore, batch prediction is very suitable for their business case. In the end, the fraud detection is run a nightly-cron job (every 24 hours).\n",
    "\n",
    "### 4.2. Horisontal scaling\n",
    "\n",
    "Horisontal scaling refers to the process of accomodating growing volume of computing tasks by spreading it over large number of commodity hardware. This methodology is the key to scale your infrastructure in an efficient and effective manner. Horisontal scaling fits data mining model deployments really well as many predictive tasks are stateless (e.g. each prediction/row are considered to be unrelated from one to another), which mean large number of predictions can easily be distributed over many models.\n",
    "\n",
    "### 4.1. Testing and retraining\n",
    "\n",
    "After a deployment, a model must be monitored periodically. As training often took place using a snapshot of training data, the model prediction quality typically degrades as the environment shifts away from the conditions that were originally captured. After a certain timeframe, the error could grow to be significant, thus the model has to be retrained or replaced.\n",
    "\n",
    "In this condition, there are two common methods to update the model. First, you could periodically collect newer data and train a \"challenger\" model. Both old model and the new challenger are then tested on new stream of data. If the challenger manages to outperform the older model, this new model and its pipeline are then deployed.\n",
    "\n",
    "The second paradigm is to use online training. In online training, newer data that becomes available in a sequential order is used to train and update model, fifferent from batch learning which waits for a large set of training data. Online learning is commonly used for training a model over a very large dataset that does not fit the memory. Online trained models are also capable of adapting to newer patterns in data, leading to common usage in time-related tasks such as stock price predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
